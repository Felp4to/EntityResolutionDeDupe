{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f7dfaca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv \n",
    "import pandas\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcca97e9",
   "metadata": {},
   "source": [
    "# Readme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281d8b9f",
   "metadata": {},
   "source": [
    "### Aggiungere directory con nome \"{NomeSorgente}_original\" contenenti i dataset raccolti per quella\n",
    "### sorgente, nella cartella 'Datasets Original'. Il nome di ogni singolo file deve essere\n",
    "### \"{NomeSorgente}_{TeamDiLavoro}.{json | jsonl | csv}\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056f4e64",
   "metadata": {},
   "source": [
    "# Input Panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7de1216c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"./Datasets Original/\"\n",
    "PATH = \"./Datasets Original/\"\n",
    "PATH2 = \"./Datasets Mediated/\"\n",
    "\n",
    "# mediated structure \n",
    "MEDIATED_STRUCTURE = {\n",
    "                \"id\": \"\",\n",
    "                \"name\":\"\",\n",
    "                \"headquarters\": \"\",\n",
    "                \"country\":\"\",\n",
    "                \"continent\":\"\",\n",
    "                \"region\": \"\",\n",
    "                \"sub_region\": \"\",\n",
    "                \"founded\":\"\",\n",
    "                \"employees\":\"\",\n",
    "                \"ceo\":\"\",\n",
    "                \"market_cap\":\"\",\n",
    "                \"categories\":\"\",\n",
    "                \"sector\":\"\",\n",
    "                \"industry\":\"\",\n",
    "                \"code\":\"\",\n",
    "                \"rank\":\"\",\n",
    "                \"revenue\":\"\",\n",
    "                \"net_income\":\"\",\n",
    "                \"link\":\"\",\n",
    "                \"share_price\":\"\",\n",
    "                \"change_1_day\": \"\",\n",
    "                \"change_1_year\": \"\",\n",
    "                \"results_for_year\": \"\",\n",
    "                \"total_assets\": \"\",\n",
    "                \"total_liabilities\": \"\",\n",
    "                \"total_equity\": \"\",\n",
    "                \"founders\": \"\",\n",
    "                \"market_value\": \"\",\n",
    "                \"address\": \"\",\n",
    "                \"type\": \"\"\n",
    "            }\n",
    "\n",
    "# match attributes\n",
    "MATCH_ATTRIBUTES = {'name': 'name',\n",
    "                    'Name': 'name',\n",
    "                    'BRAND NAME': 'name',\n",
    "                    'Company': 'name',\n",
    "                    'company': 'name',\n",
    "                    #\n",
    "                    'country': 'country',\n",
    "                    'Country': 'country',\n",
    "                    'headquarters country': 'country',\n",
    "                    'Location': 'country',\n",
    "                    'headquarters_country': 'country',\n",
    "                    #\n",
    "                    'stock': 'stock',\n",
    "                    'Stock': 'stock',\n",
    "                    #\n",
    "                    'sector': 'sector',\n",
    "                    'Sector': 'sector',\n",
    "                    'company_type': 'sector',\n",
    "                    'Company Type': 'sector',\n",
    "                    #\n",
    "                    'industry': 'industry',\n",
    "                    'Industry': 'industry',\n",
    "                    #\n",
    "                    'market_capitalization_USD': 'market_cap_usd',\n",
    "                    #\n",
    "                    'founded': 'founded',\n",
    "                    'Founded': 'founded',\n",
    "                    'Founded Year': 'founded',\n",
    "                    'IPO Year': 'founded',\n",
    "                    'founding_year': 'founded',\n",
    "                    'Registration Date': 'founded',\n",
    "                    'Family ownership': 'founded',\n",
    "                    #\n",
    "                    'employees': 'employees',\n",
    "                    'Employees': 'employees',\n",
    "                    'Number of Employees': 'employees',\n",
    "                    'number of employees': 'employees',\n",
    "                    'number_of_employees': 'employees',\n",
    "                    #\n",
    "                    'ceo':'ceo',\n",
    "                    'CEO':'ceo',\n",
    "                    #\n",
    "                    'marketcap': 'market_cap',\n",
    "                    'MasterCap': 'market_cap',\n",
    "                    'MarketCap': 'market_cap',\n",
    "                    'marketCap': 'market_cap',\n",
    "                    'pricecap': 'market_cap',\n",
    "                    'Master Cap': 'market_cap',\n",
    "                    'market_capitalization_2022': 'market_cap',\n",
    "                    'valuation': 'market_cap',\n",
    "                    #\n",
    "                    'share_price':'share_price',\n",
    "                    'sharePrice': 'share_price',\n",
    "                    'Share Price': 'share_price',\n",
    "                    #\n",
    "                    'change_1_day': 'change_1_day',\n",
    "                    'change1d': 'change_1_day',\n",
    "                    'change(1day)': 'change_1_day',\n",
    "                    #\n",
    "                    'change_1_year': 'change_1_year',\n",
    "                    'change1y': 'change_1_year',\n",
    "                    'change(1year)': 'change_1_year',\n",
    "                    #\n",
    "                    'code': 'code',\n",
    "                    'Symbol': 'code',\n",
    "                    #\n",
    "                    'rank': 'rank',\n",
    "                    'world_rank': 'rank',\n",
    "                    'world rank (Jan-07-2022)': 'rank',\n",
    "                    #\n",
    "                    'revenue': 'revenue',\n",
    "                    'annual_revenue_in_usd': 'revenue',\n",
    "                    'annual revenue in USD': 'revenue',\n",
    "                    #\n",
    "                    'net income': 'net_income',\n",
    "                    'annual_net_income_in_usd': 'net_income',\n",
    "                    'annual net income in USD': 'net_income',\n",
    "                    #\n",
    "                    'link': 'link',\n",
    "                    #\n",
    "                    'official_name': 'official_name',\n",
    "                    #\n",
    "                    'headquarters_continent': 'continent',\n",
    "                    'continent': 'continent',\n",
    "                    'Continent': 'continent',\n",
    "                    #\n",
    "                    'Headquarters': 'headquarters',\n",
    "                    'headquarters': 'headquarters',\n",
    "                    'headQuarters': 'headquarters',\n",
    "                    #\n",
    "                    'categories': 'categories',\n",
    "                    'company_business': 'categories',\n",
    "                    'company business': 'categories',\n",
    "                    'CATEGORY': 'categories',\n",
    "                    #\n",
    "                    'Top Competitor': 'top_competitor',\n",
    "                    #\n",
    "                    'annual_results_for_year_ending': 'results_for_year',\n",
    "                    #\n",
    "                    'total_assets_in_usd': 'total_assets',\n",
    "                    #\n",
    "                    'total_liabilities_in_usd': 'total_liabilities',\n",
    "                    #\n",
    "                    'total_equity_in_usd': 'total_equity',\n",
    "                    #\n",
    "                    'headquarters_region_city': 'region',\n",
    "                    #\n",
    "                    'headquarters_sub_region': 'sub_region',\n",
    "                    #\n",
    "                    'headquarters_continent': 'continent',\n",
    "                    #\n",
    "                    'founders': 'founders',\n",
    "                    #\n",
    "                    'company_website': 'company_website',\n",
    "                    'company website': 'company_website',\n",
    "                    #\n",
    "                    'market value (Jan 1st 2020)': 'market_value_2020',\n",
    "                    #\n",
    "                    'market value (Jan-07-2022)': 'market_value_2022',\n",
    "                    #\n",
    "                    'world rank (Jan-2020)': 'rank_2020',\n",
    "                    #\n",
    "                    'market_value_jan_2021': 'market_value_2021',\n",
    "                    #'': '',\n",
    "                    'address': 'address',\n",
    "                    'registered_office_address': 'address',\n",
    "                    'Office Address': 'address',\n",
    "                    #\n",
    "                    'type': 'Public/Private',\n",
    "                    'type': 'type'\n",
    "                  }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1b35e3",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "317d098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate json mediated\n",
    "def create_json_mediated(json_originale):\n",
    "    json_mediato = dict(MEDIATED_STRUCTURE)\n",
    "    for jj in json_originale:\n",
    "        # check attribute matching\n",
    "        if(jj in MATCH_ATTRIBUTES):\n",
    "            json_mediato[MATCH_ATTRIBUTES[jj]] = json_originale[jj]\n",
    "    json_mediato['id'] = str(uuid.uuid4())\n",
    "    return json_mediato\n",
    "   \n",
    "# generate file .jsonl mediated\n",
    "def create_ds_mediated(fileName, dataset):\n",
    "    array_jsons_mediati = []\n",
    "    # extension's file\n",
    "    file_name, file_extension = os.path.splitext(fileName)\n",
    "    # case 1: extension .json\n",
    "    if(file_extension == '.json'):\n",
    "        file_originale = open(PATH + dataset + \"_original\" + \"/\" + fileName, 'r', encoding=\"utf8\")\n",
    "        data = json.load(file_originale)\n",
    "        for json_azienda in data:\n",
    "            am = create_json_mediated(json_azienda)\n",
    "            array_jsons_mediati.append(am)\n",
    "        res = json.dumps(array_jsons_mediati, indent = 4)\n",
    "        file_mediato = open(PATH2 + dataset + \"_mediated\" + \"/\" + file_name + \"_m\" + \".json\", \"w\", encoding=\"utf8\")\n",
    "        file_mediato.write(str(res))\n",
    "        file_mediato.close()\n",
    "        file_originale.close()\n",
    "    # case 2: extension .jsonl\n",
    "    elif(file_extension == '.jsonl'):\n",
    "        file_originale = open(PATH + dataset + \"_original\" + \"/\" + fileName, 'r', encoding=\"utf8\")\n",
    "        for line in file_originale:\n",
    "            json_azienda = json.loads(line)\n",
    "            am = create_json_mediated(json_azienda)\n",
    "            array_jsons_mediati.append(am)\n",
    "        res = json.dumps(array_jsons_mediati, indent = 4)\n",
    "        file_mediato = open(PATH2 + dataset + \"_mediated\" + \"/\" + file_name + \"_m\" + \".json\", \"w\", encoding=\"utf8\")\n",
    "        file_mediato.write(res)\n",
    "        file_mediato.close()\n",
    "        file_originale.close()\n",
    "    elif(file_extension == '.csv'):\n",
    "        jsonArray = []\n",
    "        f = open(PATH + dataset + \"_original\" + \"/\" + fileName, 'r', encoding = 'unicode_escape')\n",
    "        file_originale_jsons = csv.DictReader(f)\n",
    "        #convert each csv row into python dict\n",
    "        for row in file_originale_jsons: \n",
    "            am = create_json_mediated(row)\n",
    "            array_jsons_mediati.append(am)\n",
    "        res = json.dumps(array_jsons_mediati, indent = 4)\n",
    "        file_mediato = open(PATH2 + dataset + \"_mediated\" + \"/\" + file_name + \"_m\" + \".json\", \"w\", encoding=\"utf8\")\n",
    "        file_mediato.write(str(res))\n",
    "        file_mediato.close()\n",
    "    # elif(file_extension == '.xlsx'):   \n",
    "    else:\n",
    "        print(\"Estensione \", file_extension, \" non supportata\")\n",
    "    \n",
    "# return jsons number\n",
    "def countJson(stringa):\n",
    "    jsons = json.leads(stringa)\n",
    "    count = 0\n",
    "    for j in jsons:\n",
    "        count = count + 1\n",
    "    print(count)\n",
    "    \n",
    "# return files list\n",
    "def find_all_files(path_dir):\n",
    "    # list to store files\n",
    "    res = []\n",
    "    # Iterate directory\n",
    "    for path in os.listdir(path_dir):\n",
    "        # check if current path is a file\n",
    "        if os.path.isfile(os.path.join(path_dir, path)):\n",
    "            res.append(path)\n",
    "    return res\n",
    "\n",
    "# find folders list \n",
    "def find_all_folders(path):\n",
    "    return os.listdir(path)\n",
    "\n",
    "# create folder\n",
    "def create_folder(name, path):\n",
    "    dir = os.path.join(path, name)\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)\n",
    "\n",
    "# GENERATE ALL MEDIATED DATASET\n",
    "def generate_all_mediated_ds():\n",
    "    folders_original = find_all_folders(ROOT)\n",
    "    for folder in folders_original:\n",
    "        # print(folder)\n",
    "        # create folder mediated\n",
    "        folder_name_mediated = folder.replace('_original', '_mediated')\n",
    "        create_folder(folder_name_mediated, PATH2)\n",
    "        # per ogni file \n",
    "        files = find_all_files(ROOT + folder + \"/\")\n",
    "        #print(find_all_files(ROOT + folder + \"/\"))\n",
    "        for fileName in files:\n",
    "            # print(fileName)\n",
    "            dataset = folder.replace('_original', '')\n",
    "            create_ds_mediated(fileName, dataset)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cb65bcf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Invalid control character at: line 1616 column 33 (char 52422)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[1;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgenerate_all_mediated_ds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [55]\u001b[0m, in \u001b[0;36mgenerate_all_mediated_ds\u001b[1;34m()\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fileName \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# print(fileName)\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m folder\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_original\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 99\u001b[0m     \u001b[43mcreate_ds_mediated\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [55]\u001b[0m, in \u001b[0;36mcreate_ds_mediated\u001b[1;34m(fileName, dataset)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(file_extension \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     18\u001b[0m     file_originale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(PATH \u001b[38;5;241m+\u001b[39m dataset \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_original\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m fileName, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124municode_escape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_originale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m json_azienda \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m     21\u001b[0m         am \u001b[38;5;241m=\u001b[39m create_json_mediated(json_azienda)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(fp\u001b[38;5;241m.\u001b[39mread(),\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Invalid control character at: line 1616 column 33 (char 52422)"
     ]
    }
   ],
   "source": [
    "generate_all_mediated_ds()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393c6a84",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62c0ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d9af28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0d355c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7275bc2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05f43c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6fdec641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed422460",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae9039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988630fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
