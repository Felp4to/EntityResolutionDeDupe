{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Record Linkage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dedupe\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output_file = 'data_matching_output.csv'\n",
    "settings_file = 'data_matching_learned_settings'\n",
    "training_file = 'data_matching_training.json'\n",
    "left_file = './Mediated Datasets/disfold_DeBiGa_mediated.csv'\n",
    "right_file = './Mediated Datasets/CompaniesMarketCap_GioPonSpiz_mediated.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pre_process(column):\n",
    "    column = unidecode(column)\n",
    "    column = re.sub('\\n', ' ', column)\n",
    "    column = re.sub('-', '', column)\n",
    "    column = re.sub('/', ' ', column)\n",
    "    column = re.sub(\"'\", '', column)\n",
    "    column = re.sub(\",\", '', column)\n",
    "    column = re.sub(\":\", ' ', column)\n",
    "    column = re.sub('  +', ' ', column)\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "    if not column:\n",
    "        column = None\n",
    "    return column\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    data_d = {}\n",
    "\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            clean_row = dict([(k, pre_process(v)) for (k, v) in row.items()])\n",
    "            data_d[filename + str(i)] = dict(clean_row)\n",
    "\n",
    "    return data_d\n",
    "\n",
    "\n",
    "def merge_rows(df):\n",
    "    unique = df[df['Cluster ID'].isnull()].reset_index(drop=True)\n",
    "    duplicates = df[~df['Cluster ID'].isnull()].reset_index(drop=True)\n",
    "    merged = duplicates.fillna('')\\\n",
    "        .groupby('Cluster ID')\\\n",
    "        .max().replace('', np.nan)\\\n",
    "        .reset_index()\n",
    "    return pd.concat([merged, unique])\\\n",
    "        .drop(['Cluster ID', 'Link Score', 'source file'], axis=1)\\\n",
    "        .reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing data ...\n",
      "reading from data_matching_learned_settings\n",
      "clustering...\n",
      "# duplicate sets 964\n"
     ]
    }
   ],
   "source": [
    "print('importing data ...')\n",
    "data_1 = read_data(left_file)\n",
    "data_2 = read_data(right_file)\n",
    "\n",
    "if os.path.exists(settings_file):\n",
    "    print('reading from', settings_file)\n",
    "    with open(settings_file, 'rb') as sf:\n",
    "        linker = dedupe.StaticRecordLink(sf)\n",
    "else:\n",
    "    fields = [\n",
    "        {'field': 'name', 'type': 'String', 'has_missing': True},\n",
    "        {'field': 'ceo', 'type': 'String', 'has missing': True}\n",
    "    ]\n",
    "    linker = dedupe.RecordLink(fields)\n",
    "\n",
    "    if os.path.exists(training_file):\n",
    "        print('reading labeled examples from ', training_file)\n",
    "        with open(training_file) as tf:\n",
    "            linker.prepare_training(data_1,\n",
    "                                    data_2,\n",
    "                                    training_file=tf,\n",
    "                                    sample_size=15)\n",
    "    else:\n",
    "        linker.prepare_training(data_1, data_2, sample_size=15)\n",
    "\n",
    "    print('starting active labeling...')\n",
    "\n",
    "    dedupe.console_label(linker)\n",
    "\n",
    "    linker.train()\n",
    "\n",
    "    with open(training_file, 'w') as tf:\n",
    "        linker.write_training(tf)\n",
    "\n",
    "    with open(settings_file, 'wb') as sf:\n",
    "        linker.write_settings(sf)\n",
    "\n",
    "print('clustering...')\n",
    "linked_records = linker.join(data_1, data_2, 0.0)\n",
    "\n",
    "print('# duplicate sets', len(linked_records))\n",
    "\n",
    "cluster_membership = {}\n",
    "for cluster_id, (cluster, score) in enumerate(linked_records):\n",
    "    for record_id in cluster:\n",
    "        cluster_membership[record_id] = {'Cluster ID': cluster_id,\n",
    "                                         'Link Score': score}\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    header_unwritten = True\n",
    "\n",
    "    for fileno, filename in enumerate((left_file, right_file)):\n",
    "        with open(filename) as f_input:\n",
    "            reader = csv.DictReader(f_input)\n",
    "\n",
    "            if header_unwritten:\n",
    "                fieldnames = (['Cluster ID', 'Link Score', 'source file'] +\n",
    "                              reader.fieldnames)\n",
    "\n",
    "                writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "\n",
    "                header_unwritten = False\n",
    "\n",
    "            for row_id, row in enumerate(reader):\n",
    "                record_id = filename + str(row_id)\n",
    "                cluster_details = cluster_membership.get(record_id, {})\n",
    "                row['source file'] = fileno\n",
    "                row.update(cluster_details)\n",
    "\n",
    "                writer.writerow(row)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mergio le informazioni dei record appartenenti alle stesse aziende"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_file, encoding='utf-8', dtype=object)\n",
    "result = merge_rows(df)\n",
    "result.to_csv('result.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}