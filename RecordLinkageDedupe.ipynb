{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record Linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dedupe\n",
    "from unidecode import unidecode\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output_file = './Data Matching Output/data_matching_output'\n",
    "settings_file = 'data_matching_learned_settings'\n",
    "training_file = 'data_matching_training.json'\n",
    "sources_path = './Mediated Datasets/'\n",
    "record_linkage_path = './Record Linkage/'\n",
    "joined_dataset = './Joined Dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_files(path_dir):\n",
    "    res = []\n",
    "    for path in os.listdir(path_dir):\n",
    "        filename = os.path.join(path_dir, path)\n",
    "        if os.path.isfile(filename) and filename != '.DS_Store':\n",
    "            res.append(path)\n",
    "    return res\n",
    "\n",
    "\n",
    "def pre_process(column):\n",
    "    column = unidecode(column)\n",
    "    column = re.sub('\\n', ' ', column)\n",
    "    column = re.sub('-', '', column)\n",
    "    column = re.sub('/', ' ', column)\n",
    "    column = re.sub(\"'\", '', column)\n",
    "    column = re.sub(\",\", '', column)\n",
    "    column = re.sub(\":\", ' ', column)\n",
    "    column = re.sub('  +', ' ', column)\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "    if not column:\n",
    "        column = None\n",
    "    return column\n",
    "\n",
    "\n",
    "def read_data(filename):\n",
    "    data_d = {}\n",
    "\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            clean_row = dict([(k, pre_process(v)) for (k, v) in row.items()])\n",
    "            data_d[filename + str(i)] = dict(clean_row)\n",
    "\n",
    "    return data_d\n",
    "\n",
    "\n",
    "def merge_same_entities(df):\n",
    "    unique = df[df['Cluster ID'].isnull()].reset_index(drop=True)\n",
    "    duplicates = df[~df['Cluster ID'].isnull()].reset_index(drop=True)\n",
    "    merged = duplicates.fillna('')\\\n",
    "        .groupby('Cluster ID')\\\n",
    "        .max().replace('', np.nan)\\\n",
    "        .reset_index()\n",
    "    return pd.concat([merged, unique])\\\n",
    "        .drop(['Cluster ID', 'Link Score', 'source file'], axis=1)\\\n",
    "        .reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing: ./Mediated Datasets/disfold_iGMM_mediated.csv\n",
      "################  START ITERATION 1  ################\n",
      "importing: ./Mediated Datasets/hitHorizons_Avengers_mediated.csv\n",
      "reading from data_matching_learned_settings\n",
      "clustering...\n",
      "clustering execution time: 4.36638069152832\n",
      "# duplicate sets 30\n",
      "################  END ITERATION 1  ################\n",
      "################  START ITERATION 2  ################\n",
      "importing: ./Mediated Datasets/valuetoday_silvestri_mediated.csv\n",
      "reading from data_matching_learned_settings\n",
      "clustering...\n",
      "clustering execution time: 6.495687961578369\n",
      "# duplicate sets 229\n",
      "################  END ITERATION 2  ################\n",
      "################  START ITERATION 3  ################\n",
      "importing: ./Mediated Datasets/companiesmarketcap_iGMM_mediated.csv\n",
      "reading from data_matching_learned_settings\n",
      "clustering...\n",
      "clustering execution time: 31.577800989151\n",
      "# duplicate sets 1409\n",
      "################  END ITERATION 3  ################\n",
      "################  START ITERATION 4  ################\n",
      "importing: ./Mediated Datasets/wiki_GioPonSPiz_mediated.csv\n",
      "reading from data_matching_learned_settings\n",
      "clustering...\n",
      "clustering execution time: 46.51365303993225\n",
      "# duplicate sets 444\n",
      "################  END ITERATION 4  ################\n",
      "################  START ITERATION 5  ################\n",
      "importing: ./Mediated Datasets/valueToday_iGMM_mediated.csv\n",
      "reading from data_matching_learned_settings\n",
      "clustering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnProcess-37:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alessandro/opt/anaconda3/envs/Homework8/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/alessandro/opt/anaconda3/envs/Homework8/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/alessandro/opt/anaconda3/envs/Homework8/lib/python3.10/site-packages/dedupe/core.py\", line 79, in __call__\n",
      "    self.fieldDistance(record_pairs)\n",
      "  File \"/Users/alessandro/opt/anaconda3/envs/Homework8/lib/python3.10/site-packages/dedupe/core.py\", line 100, in fieldDistance\n",
      "    fp = numpy.memmap(\n",
      "  File \"/Users/alessandro/opt/anaconda3/envs/Homework8/lib/python3.10/site-packages/numpy/core/memmap.py\", line 255, in __new__\n",
      "    fid.flush()\n",
      "OSError: [Errno 28] No space left on device\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "database or disk is full",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOperationalError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 53\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mclustering...\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     52\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m---> 53\u001B[0m linked_records \u001B[38;5;241m=\u001B[39m \u001B[43mlinker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_2\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0.5\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mclustering execution time:\u001B[39m\u001B[38;5;124m'\u001B[39m, time\u001B[38;5;241m.\u001B[39mtime()\u001B[38;5;241m-\u001B[39mstart_time)\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m# duplicate sets\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28mlen\u001B[39m(linked_records))\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Homework8/lib/python3.10/site-packages/dedupe/api.py:549\u001B[0m, in \u001B[0;36mRecordLinkMatching.join\u001B[0;34m(self, data_1, data_2, threshold, constraint)\u001B[0m\n\u001B[1;32m    543\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m constraint \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mone-to-one\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmany-to-one\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmany-to-many\u001B[39m\u001B[38;5;124m\"\u001B[39m}, (\n\u001B[1;32m    544\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m is an invalid constraint option. Valid options include \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    545\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mone-to-one, many-to-one, or many-to-many\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m constraint\n\u001B[1;32m    546\u001B[0m )\n\u001B[1;32m    548\u001B[0m pairs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpairs(data_1, data_2)\n\u001B[0;32m--> 549\u001B[0m pair_scores \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscore\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpairs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    551\u001B[0m links: Links\n\u001B[1;32m    552\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m constraint \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mone-to-one\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Homework8/lib/python3.10/site-packages/dedupe/api.py:125\u001B[0m, in \u001B[0;36mIntegralMatching.score\u001B[0;34m(self, pairs)\u001B[0m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;124;03mScores pairs of records. Returns pairs of tuples of records id and\u001B[39;00m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;124;03massociated probabilities that the pair of records are match\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    122\u001B[0m \n\u001B[1;32m    123\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 125\u001B[0m     matches \u001B[38;5;241m=\u001B[39m \u001B[43mcore\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscoreDuplicates\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    126\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpairs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistances\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclassifier\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_cores\u001B[49m\n\u001B[1;32m    127\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m:\n\u001B[1;32m    129\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    130\u001B[0m         \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    131\u001B[0m \u001B[38;5;124;03m        You need to either turn off multiprocessing or protect\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;124;03m        https://docs.python.org/3/library/multiprocessing.html#the-spawn-and-forkserver-start-methods\"\"\"\u001B[39;00m\n\u001B[1;32m    135\u001B[0m     )\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Homework8/lib/python3.10/site-packages/dedupe/core.py:160\u001B[0m, in \u001B[0;36mscoreDuplicates\u001B[0;34m(record_pairs, featurizer, classifier, num_cores)\u001B[0m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m process \u001B[38;5;129;01min\u001B[39;00m map_processes:\n\u001B[1;32m    158\u001B[0m     process\u001B[38;5;241m.\u001B[39mstart()\n\u001B[0;32m--> 160\u001B[0m \u001B[43mfillQueue\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrecord_pairs_queue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrecord_pairs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_map_processes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m process \u001B[38;5;129;01min\u001B[39;00m map_processes:\n\u001B[1;32m    163\u001B[0m     process\u001B[38;5;241m.\u001B[39mjoin()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Homework8/lib/python3.10/site-packages/dedupe/core.py:188\u001B[0m, in \u001B[0;36mfillQueue\u001B[0;34m(queue, iterable, stop_signals, chunk_size)\u001B[0m\n\u001B[1;32m    185\u001B[0m iterable \u001B[38;5;241m=\u001B[39m \u001B[38;5;28miter\u001B[39m(iterable)\n\u001B[1;32m    187\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 188\u001B[0m     chunk \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mitertools\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mislice\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    189\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m chunk:\n\u001B[1;32m    190\u001B[0m         queue\u001B[38;5;241m.\u001B[39mput(chunk)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Homework8/lib/python3.10/site-packages/dedupe/api.py:463\u001B[0m, in \u001B[0;36mRecordLinkMatching.pairs\u001B[0;34m(self, data_1, data_2)\u001B[0m\n\u001B[1;32m    454\u001B[0m con\u001B[38;5;241m.\u001B[39mexecute(\u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;124mANALYZE\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m)\n\u001B[1;32m    456\u001B[0m pairs \u001B[38;5;241m=\u001B[39m con\u001B[38;5;241m.\u001B[39mexecute(\n\u001B[1;32m    457\u001B[0m     \u001B[38;5;124;03m\"\"\"SELECT DISTINCT a.record_id, b.record_id\u001B[39;00m\n\u001B[1;32m    458\u001B[0m \u001B[38;5;124;03m                       FROM blocking_map_a a\u001B[39;00m\n\u001B[1;32m    459\u001B[0m \u001B[38;5;124;03m                       INNER JOIN blocking_map_b b\u001B[39;00m\n\u001B[1;32m    460\u001B[0m \u001B[38;5;124;03m                       USING (block_key)\"\"\"\u001B[39;00m\n\u001B[1;32m    461\u001B[0m )\n\u001B[0;32m--> 463\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m a_record_id, b_record_id \u001B[38;5;129;01min\u001B[39;00m pairs:\n\u001B[1;32m    464\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m (\n\u001B[1;32m    465\u001B[0m         (a_record_id, data_1[a_record_id]),\n\u001B[1;32m    466\u001B[0m         (b_record_id, data_2[b_record_id]),\n\u001B[1;32m    467\u001B[0m     )\n\u001B[1;32m    469\u001B[0m pairs\u001B[38;5;241m.\u001B[39mclose()\n",
      "\u001B[0;31mOperationalError\u001B[0m: database or disk is full"
     ]
    }
   ],
   "source": [
    "files = get_all_files(sources_path)\n",
    "left_file = sources_path + files.pop(0)\n",
    "print('importing:', left_file)\n",
    "data_1 = read_data(left_file)\n",
    "\n",
    "\n",
    "for n, file in enumerate(files):\n",
    "    print('################  START ITERATION ' + str(n+1) + '  ################')\n",
    "    right_file = sources_path + file\n",
    "    print('importing:', right_file)\n",
    "    data_2 = read_data(right_file)\n",
    "\n",
    "    if os.path.exists(settings_file):\n",
    "        print('reading from', settings_file)\n",
    "        with open(settings_file, 'rb') as sf:\n",
    "            linker = dedupe.StaticRecordLink(sf)\n",
    "    else:\n",
    "        fields = [\n",
    "            {'field': 'name', 'type': 'String'},\n",
    "            {'field': 'country', 'type': 'String', 'has missing': True},\n",
    "            {'field': 'ceo', 'type': 'String', 'has missing': True},\n",
    "            {'field': 'founded', 'type': 'String', 'has missing': True},\n",
    "            {'field': 'link', 'type': 'String', 'has missing': True},\n",
    "            {'field': 'address', 'type': 'String', 'has missing': True},\n",
    "            {'field': 'sector', 'type': 'String', 'has missing': True}\n",
    "        ]\n",
    "        linker = dedupe.RecordLink(fields)\n",
    "\n",
    "        if os.path.exists(training_file):\n",
    "            print('reading labeled examples from ', training_file)\n",
    "            with open(training_file) as tf:\n",
    "                linker.prepare_training(data_1,\n",
    "                                        data_2,\n",
    "                                        training_file=tf,\n",
    "                                        sample_size=150)\n",
    "        else:\n",
    "            linker.prepare_training(data_1, data_2, sample_size=150)\n",
    "\n",
    "        print('starting active labeling...')\n",
    "\n",
    "        dedupe.console_label(linker)\n",
    "\n",
    "        linker.train()\n",
    "\n",
    "        with open(training_file, 'w') as tf:\n",
    "            linker.write_training(tf)\n",
    "\n",
    "        with open(settings_file, 'wb') as sf:\n",
    "            linker.write_settings(sf)\n",
    "\n",
    "    print('clustering...')\n",
    "    start_time = time.time()\n",
    "    linked_records = linker.join(data_1, data_2, 0.5)\n",
    "    print('clustering execution time:', time.time()-start_time)\n",
    "\n",
    "    print('# duplicate sets', len(linked_records))\n",
    "\n",
    "    cluster_membership = {}\n",
    "    for cluster_id, (cluster, score) in enumerate(linked_records):\n",
    "        for record_id in cluster:\n",
    "            cluster_membership[record_id] = {'Cluster ID': cluster_id,\n",
    "                                             'Link Score': score}\n",
    "\n",
    "    with open(output_file + '_' + str(n) + '.csv', 'w') as f:\n",
    "        header_unwritten = True\n",
    "\n",
    "        for fileno, filename in enumerate((left_file, right_file)):\n",
    "            with open(filename) as f_input:\n",
    "                reader = csv.DictReader(f_input)\n",
    "\n",
    "                if header_unwritten:\n",
    "                    fieldnames = (['Cluster ID', 'Link Score', 'source file'] +\n",
    "                                  reader.fieldnames)\n",
    "\n",
    "                    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "                    writer.writeheader()\n",
    "\n",
    "                    header_unwritten = False\n",
    "\n",
    "                for row_id, row in enumerate(reader):\n",
    "                    record_id = filename + str(row_id)\n",
    "                    cluster_details = cluster_membership.get(record_id, {})\n",
    "                    row['source file'] = fileno\n",
    "                    row.update(cluster_details)\n",
    "                    writer.writerow(row)\n",
    "\n",
    "    ### Mergio le informazioni dei record appartenenti alle stesse aziende\n",
    "    df = pd.read_csv(output_file + '_' + str(n) + '.csv', encoding='utf-8', dtype=object)\n",
    "    result = merge_same_entities(df)\n",
    "    if n == (len(files)-1):\n",
    "        left_file = record_linkage_path + 'final_result_merge.csv'\n",
    "    else:\n",
    "        left_file = record_linkage_path + 'result_merge' + '_' + str(n) + '.csv'\n",
    "    result.to_csv(left_file, index=False)\n",
    "    data_1 = read_data(left_file)\n",
    "    print('################  END ITERATION ' + str(n+1) + '  ################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}